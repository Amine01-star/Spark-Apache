{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d31deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\administrateur\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\administrateur\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c03fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69ca9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae241fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-RAGD177R:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e1ead106a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61510680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2657436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------+-------------------+----------+\n",
      "|idTransaction|nomClient|quantiteAchete|          dateAchat|row_number|\n",
      "+-------------+---------+--------------+-------------------+----------+\n",
      "|      9788460|  Barrett|            12|1970-01-03 00:00:00|         1|\n",
      "|      2394878|    Grant|            12|1970-01-04 00:00:00|         2|\n",
      "|      3645136|  Jackson|            12|1970-01-05 00:00:00|         3|\n",
      "|      5884156|  Frazier|            12|1970-01-07 00:00:00|         4|\n",
      "|      1560813|     Page|            12|1970-01-09 00:00:00|         5|\n",
      "|      5833095|  Russell|            12|1970-01-09 00:00:00|         6|\n",
      "|      8565782|    Booth|            12|1970-01-11 00:00:00|         7|\n",
      "|      6424499|  Johnson|            12|1970-01-12 00:00:00|         8|\n",
      "|      2460489|   Garcia|            12|1970-01-13 00:00:00|         9|\n",
      "|      2985467|   Sutton|            12|1970-01-14 00:00:00|        10|\n",
      "|      2086909|   Palmer|            12|1970-01-15 00:00:00|        11|\n",
      "|      4094468|Alexander|            12|1970-01-17 00:00:00|        12|\n",
      "|      4451790|    Glass|            12|1970-01-17 00:00:00|        13|\n",
      "|      1699084| Gonzalez|            12|1970-01-19 00:00:00|        14|\n",
      "|      8194474|   Garcia|            12|1970-01-20 00:00:00|        15|\n",
      "|      8943043|   Warner|            12|1970-01-20 00:00:00|        16|\n",
      "|      9033491| Crawford|            12|1970-01-20 00:00:00|        17|\n",
      "|      9032603|  Burgess|            12|1970-01-21 00:00:00|        18|\n",
      "|      1823103|   Conley|            12|1970-01-22 00:00:00|        19|\n",
      "|      9049233|   Mendez|            12|1970-01-23 00:00:00|        20|\n",
      "+-------------+---------+--------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff7=spark.read.csv(\n",
    "    path=\"spark.dat\",\n",
    "    sep=\";\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "\n",
    "#dff7.show(3,truncate=False)\n",
    "dff7.columns\n",
    "\n",
    "#dff7.select(['nomClient','quantiteAchete','prixUnitaire','ChiffreDDD']).show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "#cette requete permet de savoir les achats les plus anciens en se basant sur la date d'achat et la quantité achetée\n",
    "dff7=dff7.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(dff7['quantiteAchete']).orderBy(dff7['dateAchat'])))\n",
    "dff7.select(['idTransaction','nomClient','quantiteAchete','dateAchat','row_number']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0745ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "|idTransaction|prenomClient|nomClient|        metierClient|plaqueImmatriculationClient|numeroCarteBleuClient|dateExpirationCarteBleuClient|codeSecuriteCarteBleuClient|codeBarProduit|    motDePasseClient|adresseIPClient|quantiteAchete|prixUnitaire|          dateAchat|nomFilialeAyantVendueLeProduit|row_number|ChiffreDDD|\n",
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "|      1100765|     Anthony| Gonzales| Animal nutritionist|                    GPA 482|     6535053079899126|                        01/28|                        243| 5550967481886|o445cvlh8ydky8x2s...|  115.15.148.46|            31|           8|1970-01-01 00:00:00|               Hernandez Group|         1|       248|\n",
      "|      4411883|       Kevin|     Mack|Human resources o...|                    MJN 832|     4671624743880699|                        05/23|                        401| 4834270846244|9ksgx3rteozw3kwju...|   96.48.26.115|            31|          42|1970-01-01 00:00:00|          Smith, Bartlett a...|         2|      1302|\n",
      "|      3612557|      Sharon|     Kirk|Amenity horticult...|                       323F|     2394319933662979|                        12/31|                       5507| 5757213113410|7w4sf1jgx0oxt4fpa...|   73.235.10.39|            31|          25|1970-01-03 00:00:00|                     King-Vega|         3|       775|\n",
      "|      4086266|     Kathryn|    Smith|             Barista|                    251 OTM|     6566281701855388|                        08/25|                        479| 8236481228363|kxybm7ffxjx6k5ewh...| 108.63.203.232|            31|          13|1970-01-03 00:00:00|                 Goodman Group|         4|       403|\n",
      "|      4345096|       Laura|  Barnett|Consulting civil ...|                    MZS-200|        4716347365071|                        04/27|                        479| 1974519323563|eik0xm6jvzi8lvc6v...|   136.4.57.195|            31|           4|1970-01-05 00:00:00|                Galvan-Charles|         5|       124|\n",
      "|      4817897|     Michele|    Davis|Psychologist, cou...|                    3-6717C|     3562721467924838|                        09/31|                        505| 2468015701226|qfmr89engoc5p4mze...|   24.7.206.244|            31|          28|1970-01-06 00:00:00|                  King-Parrish|         6|       868|\n",
      "|      1474622|       Laura|   Norman|Clinical molecula...|                    27-L349|        4061750693440|                        12/31|                        536| 3337662029481|wt66z8xfh3x5e5q5j...| 166.81.219.159|            31|          28|1970-01-07 00:00:00|                   Brown-Jones|         7|       868|\n",
      "|      6423445|        Paul|  Salinas|Armed forces logi...|                    KSP 812|     3535486889776943|                        03/23|                        850| 7500998549886|gfqi00ybv86wotmob...|    25.168.8.38|            31|          41|1970-01-07 00:00:00|                    Garcia LLC|         8|      1271|\n",
      "|      8463276|       Jacob|    Hardy|        Geoscientist|                    HVW 100|     3516377393072015|                        07/31|                        355| 9256016305696|kku2sge3klnadzi55...| 165.199.126.69|            31|          17|1970-01-08 00:00:00|                     Stone LLC|         9|       527|\n",
      "|       204511|        Lisa| Crawford|    Sports therapist|                    6BO3932|         676279009945|                        07/26|                        259| 8540796717475|31n2g8v1cdmhi3pw6...| 158.116.35.207|            31|          30|1970-01-09 00:00:00|                   Petty-Blair|        10|       930|\n",
      "|      4663504|      Vickie|  Pearson|Psychologist, pri...|                    VD 1131|        4369451614445|                        09/30|                        493| 6961681819937|6q7uzpcx9ikmktzum...| 70.139.105.249|            31|          33|1970-01-09 00:00:00|                Lester-Barnett|        11|      1023|\n",
      "|      5098040|      Olivia|   Stuart| Retail merchandiser|                    ZPH 060|     3598370337522758|                        10/25|                        853| 7912980148173|vla7qoomm3qvx7ddj...| 123.17.136.185|            31|          14|1970-01-09 00:00:00|          Nguyen, Mckenzie ...|        12|       434|\n",
      "|      5590769|      Andrea|    Burke|Social research o...|                    208 VW3|     4000566596510232|                        08/32|                        950| 1581212720522|gscg9ixrm3syo5807...|  185.162.28.88|            31|          50|1970-01-09 00:00:00|                Reese and Sons|        13|      1550|\n",
      "|      2310510|      Latoya|   Prince|Scientist, resear...|                    SSN-034|       38927096788724|                        10/25|                       9997| 8910249567009|kuqbiywg2w9pr3l7w...| 107.193.73.115|            31|          47|1970-01-10 00:00:00|                    Pierce PLC|        14|      1457|\n",
      "|      4300883|        John|   Taylor|Commercial/reside...|                    LO5 B3W|      346316144643254|                        02/32|                        700| 6593148223478|el9p1396abxy7hfvr...| 179.81.163.163|            31|          34|1970-01-10 00:00:00|                    Benton LLC|        15|      1054|\n",
      "|      4912996|       Jimmy|   Jacobs|  Arts administrator|                    035 JWW|     4589632160584968|                        12/24|                        615| 7514391418670|7ah46kpwc7m2d4s4z...|  93.254.252.42|            31|          38|1970-01-12 00:00:00|                 Turner-Taylor|        16|      1178|\n",
      "|      7661295|     William|    Nolan|   Financial adviser|                    246 PUU|         676249527745|                        11/27|                        617| 6387761960328|zsskgbgx32ceaqa75...| 133.183.85.147|            31|          27|1970-01-13 00:00:00|               Castro and Sons|        17|       837|\n",
      "|      6997291|        Erin|    Blair|        Psychiatrist|                    34Q A83|      180039597338664|                        06/26|                        859| 3194231734982|h7hv1uf7ozy7e2ns1...| 203.200.40.210|            31|          34|1970-01-14 00:00:00|          Brandt, Thomas an...|        18|      1054|\n",
      "|       600953|     Gregory|   Clarke|Surveyor, plannin...|                    936-UVO|      344239201522215|                        09/27|                        937| 7621378278674|jcpj4sd2gv1ke1wkv...| 141.250.158.40|            31|          22|1970-01-15 00:00:00|          Graham, Powell an...|        19|       682|\n",
      "|      1287004|     Jeffrey|   Rivera|      Radio producer|                    149 1YI|       30183928002151|                        03/30|                        443|  345700699179|9pdhh2fp1osu9v9zg...|137.177.148.213|            31|          27|1970-01-16 00:00:00|          Pacheco, Shields ...|        20|       837|\n",
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+\n",
      "|sum(ChiffreDDD)|\n",
      "+---------------+\n",
      "|    63894693298|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#le chiffre d'affaires par client\n",
    "dff7=dff7.withColumn(\"ChiffreDDD\",dff7[\"quantiteAchete\"]*dff7[\"prixUnitaire\"])\n",
    "dff7.show()\n",
    "#le chiffre d'affaires total\n",
    "dff7.agg({'ChiffreDDD':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46bdf4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sum(ChiffreDDD)|\n",
      "+---------------+\n",
      "|    63894693298|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['idTransaction',\n",
       " 'prenomClient',\n",
       " 'nomClient',\n",
       " 'metierClient',\n",
       " 'plaqueImmatriculationClient',\n",
       " 'numeroCarteBleuClient',\n",
       " 'dateExpirationCarteBleuClient',\n",
       " 'codeSecuriteCarteBleuClient',\n",
       " 'codeBarProduit',\n",
       " 'motDePasseClient',\n",
       " 'adresseIPClient',\n",
       " 'quantiteAchete',\n",
       " 'prixUnitaire',\n",
       " 'dateAchat',\n",
       " 'nomFilialeAyantVendueLeProduit',\n",
       " 'row_number',\n",
       " 'ChiffreDDD']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifier si la colonne du chiffre d'affaires ChiffreDDD a été ajoutée.\n",
    "dff7.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11048bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idTransaction: integer (nullable = true)\n",
      " |-- prenomClient: string (nullable = true)\n",
      " |-- nomClient: string (nullable = true)\n",
      " |-- metierClient: string (nullable = true)\n",
      " |-- plaqueImmatriculationClient: string (nullable = true)\n",
      " |-- numeroCarteBleuClient: long (nullable = true)\n",
      " |-- dateExpirationCarteBleuClient: string (nullable = true)\n",
      " |-- codeSecuriteCarteBleuClient: integer (nullable = true)\n",
      " |-- codeBarProduit: long (nullable = true)\n",
      " |-- motDePasseClient: string (nullable = true)\n",
      " |-- adresseIPClient: string (nullable = true)\n",
      " |-- quantiteAchete: integer (nullable = true)\n",
      " |-- prixUnitaire: integer (nullable = true)\n",
      " |-- dateAchat: timestamp (nullable = true)\n",
      " |-- nomFilialeAyantVendueLeProduit: string (nullable = true)\n",
      " |-- row_number: integer (nullable = false)\n",
      " |-- ChiffreDDD: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ça permet de voir le type de données dans chaque colonne\n",
    "dff7.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec132b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          dateAchat|\n",
      "+-------------------+\n",
      "|1980-12-31 00:00:00|\n",
      "|2005-02-07 00:00:00|\n",
      "|1972-05-09 00:00:00|\n",
      "|1980-04-01 00:00:00|\n",
      "|1985-12-07 00:00:00|\n",
      "|2009-06-14 00:00:00|\n",
      "|2001-11-21 00:00:00|\n",
      "|2009-04-18 00:00:00|\n",
      "|2017-08-01 00:00:00|\n",
      "|1980-06-13 00:00:00|\n",
      "|2021-06-24 00:00:00|\n",
      "|2020-10-24 00:00:00|\n",
      "|2018-12-28 00:00:00|\n",
      "|2019-02-14 00:00:00|\n",
      "|2011-11-02 00:00:00|\n",
      "|1984-03-13 00:00:00|\n",
      "|1974-11-23 00:00:00|\n",
      "|2014-02-04 00:00:00|\n",
      "|2019-08-21 00:00:00|\n",
      "|2013-04-17 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- idTransaction: integer (nullable = true)\n",
      " |-- prenomClient: string (nullable = true)\n",
      " |-- nomClient: string (nullable = true)\n",
      " |-- metierClient: string (nullable = true)\n",
      " |-- plaqueImmatriculationClient: string (nullable = true)\n",
      " |-- numeroCarteBleuClient: long (nullable = true)\n",
      " |-- dateExpirationCarteBleuClient: string (nullable = true)\n",
      " |-- codeSecuriteCarteBleuClient: integer (nullable = true)\n",
      " |-- codeBarProduit: long (nullable = true)\n",
      " |-- motDePasseClient: string (nullable = true)\n",
      " |-- adresseIPClient: string (nullable = true)\n",
      " |-- quantiteAchete: integer (nullable = true)\n",
      " |-- prixUnitaire: integer (nullable = true)\n",
      " |-- dateAchat: timestamp (nullable = true)\n",
      " |-- nomFilialeAyantVendueLeProduit: string (nullable = true)\n",
      " |-- row_number: integer (nullable = false)\n",
      " |-- ChiffreDDD: integer (nullable = true)\n",
      "\n",
      "+--------------+\n",
      "|codeBarProduit|\n",
      "+--------------+\n",
      "| 1452719839898|\n",
      "| 6590804059921|\n",
      "| 9011074274557|\n",
      "| 9202477920268|\n",
      "| 2670222492457|\n",
      "| 2677328919970|\n",
      "|  249035808629|\n",
      "| 4198330953519|\n",
      "| 1254006482585|\n",
      "| 3157923913966|\n",
      "|  585764669484|\n",
      "| 2634683599599|\n",
      "|  156182902882|\n",
      "| 1388520987773|\n",
      "| 4275462053840|\n",
      "| 9743234861818|\n",
      "| 1162013307237|\n",
      "|  439440719805|\n",
      "| 4879799723836|\n",
      "| 9673588628129|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cette requête permet de voir la date d'achat des produits des 20 premières lignes\n",
    "dff7.select(['dateAchat']).show()\n",
    "#permet de voir le type des données de chaque colonne\n",
    "dff7.printSchema()\n",
    "#cette requete nous permet de savoir le code barre du produit le plus acheté \n",
    "dff7.select(['codeBarProduit']).show()\n",
    "#dff7.select(['codeBarProduit']).duplicated() # sans doublons mais ça ne marche pas !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee95c25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29e72a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[idTransaction: int, prenomClient: string, nomClient: string, metierClient: string, plaqueImmatriculationClient: string, numeroCarteBleuClient: bigint, dateExpirationCarteBleuClient: string, codeSecuriteCarteBleuClient: int, codeBarProduit: bigint, motDePasseClient: string, adresseIPClient: string, quantiteAchete: int, prixUnitaire: int, dateAchat: timestamp, nomFilialeAyantVendueLeProduit: string, row_number: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Supprimer une colonne - On a supprimé la colonne du Chiffre d'affaire\n",
    "dff7.drop('ChiffreDDD') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2757e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|idTransactionNouveaunom|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      2|\n",
      "|                      3|\n",
      "|                      4|\n",
      "|                      5|\n",
      "|                      6|\n",
      "|                      7|\n",
      "|                      8|\n",
      "|                      9|\n",
      "|                     10|\n",
      "|                     11|\n",
      "|                     12|\n",
      "|                     13|\n",
      "|                     14|\n",
      "|                     15|\n",
      "|                     16|\n",
      "|                     17|\n",
      "|                     18|\n",
      "|                     19|\n",
      "|                     20|\n",
      "+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renommer une colonne\n",
    "dff7.withColumnRenamed('idTransaction', 'idTransactionNouveaunom').select(['idTransactionNouveaunom']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48b16ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "|idTransaction|prenomClient|nomClient|        metierClient|plaqueImmatriculationClient|numeroCarteBleuClient|dateExpirationCarteBleuClient|codeSecuriteCarteBleuClient|codeBarProduit|    motDePasseClient|adresseIPClient|quantiteAchete|prixUnitaire|          dateAchat|nomFilialeAyantVendueLeProduit|row_number|ChiffreDDD|\n",
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "|      1100765|     Anthony| Gonzales| Animal nutritionist|                    GPA 482|     6535053079899126|                        01/28|                        243| 5550967481886|o445cvlh8ydky8x2s...|  115.15.148.46|            31|           8|1970-01-01 00:00:00|               Hernandez Group|         1|       248|\n",
      "|      4411883|       Kevin|     Mack|Human resources o...|                    MJN 832|     4671624743880699|                        05/23|                        401| 4834270846244|9ksgx3rteozw3kwju...|   96.48.26.115|            31|          42|1970-01-01 00:00:00|          Smith, Bartlett a...|         2|      1302|\n",
      "|      3612557|      Sharon|     Kirk|Amenity horticult...|                       323F|     2394319933662979|                        12/31|                       5507| 5757213113410|7w4sf1jgx0oxt4fpa...|   73.235.10.39|            31|          25|1970-01-03 00:00:00|                     King-Vega|         3|       775|\n",
      "|      4086266|     Kathryn|    Smith|             Barista|                    251 OTM|     6566281701855388|                        08/25|                        479| 8236481228363|kxybm7ffxjx6k5ewh...| 108.63.203.232|            31|          13|1970-01-03 00:00:00|                 Goodman Group|         4|       403|\n",
      "|      4345096|       Laura|  Barnett|Consulting civil ...|                    MZS-200|        4716347365071|                        04/27|                        479| 1974519323563|eik0xm6jvzi8lvc6v...|   136.4.57.195|            31|           4|1970-01-05 00:00:00|                Galvan-Charles|         5|       124|\n",
      "|      4817897|     Michele|    Davis|Psychologist, cou...|                    3-6717C|     3562721467924838|                        09/31|                        505| 2468015701226|qfmr89engoc5p4mze...|   24.7.206.244|            31|          28|1970-01-06 00:00:00|                  King-Parrish|         6|       868|\n",
      "|      1474622|       Laura|   Norman|Clinical molecula...|                    27-L349|        4061750693440|                        12/31|                        536| 3337662029481|wt66z8xfh3x5e5q5j...| 166.81.219.159|            31|          28|1970-01-07 00:00:00|                   Brown-Jones|         7|       868|\n",
      "|      6423445|        Paul|  Salinas|Armed forces logi...|                    KSP 812|     3535486889776943|                        03/23|                        850| 7500998549886|gfqi00ybv86wotmob...|    25.168.8.38|            31|          41|1970-01-07 00:00:00|                    Garcia LLC|         8|      1271|\n",
      "|      8463276|       Jacob|    Hardy|        Geoscientist|                    HVW 100|     3516377393072015|                        07/31|                        355| 9256016305696|kku2sge3klnadzi55...| 165.199.126.69|            31|          17|1970-01-08 00:00:00|                     Stone LLC|         9|       527|\n",
      "|       204511|        Lisa| Crawford|    Sports therapist|                    6BO3932|         676279009945|                        07/26|                        259| 8540796717475|31n2g8v1cdmhi3pw6...| 158.116.35.207|            31|          30|1970-01-09 00:00:00|                   Petty-Blair|        10|       930|\n",
      "|      4663504|      Vickie|  Pearson|Psychologist, pri...|                    VD 1131|        4369451614445|                        09/30|                        493| 6961681819937|6q7uzpcx9ikmktzum...| 70.139.105.249|            31|          33|1970-01-09 00:00:00|                Lester-Barnett|        11|      1023|\n",
      "|      5098040|      Olivia|   Stuart| Retail merchandiser|                    ZPH 060|     3598370337522758|                        10/25|                        853| 7912980148173|vla7qoomm3qvx7ddj...| 123.17.136.185|            31|          14|1970-01-09 00:00:00|          Nguyen, Mckenzie ...|        12|       434|\n",
      "|      5590769|      Andrea|    Burke|Social research o...|                    208 VW3|     4000566596510232|                        08/32|                        950| 1581212720522|gscg9ixrm3syo5807...|  185.162.28.88|            31|          50|1970-01-09 00:00:00|                Reese and Sons|        13|      1550|\n",
      "|      2310510|      Latoya|   Prince|Scientist, resear...|                    SSN-034|       38927096788724|                        10/25|                       9997| 8910249567009|kuqbiywg2w9pr3l7w...| 107.193.73.115|            31|          47|1970-01-10 00:00:00|                    Pierce PLC|        14|      1457|\n",
      "|      4300883|        John|   Taylor|Commercial/reside...|                    LO5 B3W|      346316144643254|                        02/32|                        700| 6593148223478|el9p1396abxy7hfvr...| 179.81.163.163|            31|          34|1970-01-10 00:00:00|                    Benton LLC|        15|      1054|\n",
      "|      4912996|       Jimmy|   Jacobs|  Arts administrator|                    035 JWW|     4589632160584968|                        12/24|                        615| 7514391418670|7ah46kpwc7m2d4s4z...|  93.254.252.42|            31|          38|1970-01-12 00:00:00|                 Turner-Taylor|        16|      1178|\n",
      "|      7661295|     William|    Nolan|   Financial adviser|                    246 PUU|         676249527745|                        11/27|                        617| 6387761960328|zsskgbgx32ceaqa75...| 133.183.85.147|            31|          27|1970-01-13 00:00:00|               Castro and Sons|        17|       837|\n",
      "|      6997291|        Erin|    Blair|        Psychiatrist|                    34Q A83|      180039597338664|                        06/26|                        859| 3194231734982|h7hv1uf7ozy7e2ns1...| 203.200.40.210|            31|          34|1970-01-14 00:00:00|          Brandt, Thomas an...|        18|      1054|\n",
      "|       600953|     Gregory|   Clarke|Surveyor, plannin...|                    936-UVO|      344239201522215|                        09/27|                        937| 7621378278674|jcpj4sd2gv1ke1wkv...| 141.250.158.40|            31|          22|1970-01-15 00:00:00|          Graham, Powell an...|        19|       682|\n",
      "|      1287004|     Jeffrey|   Rivera|      Radio producer|                    149 1YI|       30183928002151|                        03/30|                        443|  345700699179|9pdhh2fp1osu9v9zg...|137.177.148.213|            31|          27|1970-01-16 00:00:00|          Pacheco, Shields ...|        20|       837|\n",
      "+-------------+------------+---------+--------------------+---------------------------+---------------------+-----------------------------+---------------------------+--------------+--------------------+---------------+--------------+------------+-------------------+------------------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#supprimer les lignes avec des valeurs null\n",
    "dff7.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### MLLIB ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91921165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-RAGD177R:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e1ead106a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#à partir d'ici, on va faire des requetes de la librairie spark apache MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml. feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark_MLlib\").getOrCreate()\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a33a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml. feature import VectorAssembler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4f85ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|quantiteAchete|prixUnitaire|\n",
      "+--------------+------------+\n",
      "|           198|           9|\n",
      "|           224|          16|\n",
      "|           159|           4|\n",
      "|           263|           3|\n",
      "|           184|          45|\n",
      "|           223|          24|\n",
      "|           363|          24|\n",
      "|            91|          38|\n",
      "|           354|          48|\n",
      "|           163|          41|\n",
      "|           269|          44|\n",
      "|           371|          34|\n",
      "|           499|          25|\n",
      "|            24|          13|\n",
      "|           220|           5|\n",
      "|           187|          30|\n",
      "|            25|          13|\n",
      "|           117|          38|\n",
      "|           245|          26|\n",
      "|           275|          50|\n",
      "+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lire le fichier \n",
    "dataset_1=spark.read.csv(\n",
    "    path=\"spark.dat\",\n",
    "    sep=\";\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,)\n",
    "\n",
    "dataset_1.columns\n",
    "#selectionner des colonnes avec des nombres entiers\n",
    "dataset=dataset_1.select(['quantiteAchete','prixUnitaire'])\n",
    "feat_cols= ['quantiteAchete','prixUnitaire']\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80cd21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regroupper plusieurs colonnes d'un dataframe en un seule de caractéristiques\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')\n",
    "final_data=vec_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51875106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler : permet de normaliser les données\n",
    "from pyspark.ml.feature import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57e69287",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True,withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32afcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerModel = scaler.fit(final_data)\n",
    "cluster_final_data = scalerModel.transform(final_data)\n",
    "cluster_final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ffc8959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+--------------------+\n",
      "|quantiteAchete|prixUnitaire|    features|      scaledFeatures|\n",
      "+--------------+------------+------------+--------------------+\n",
      "|           198|           9| [198.0,9.0]|[1.37170670925851...|\n",
      "|           224|          16|[224.0,16.0]|[1.55182981249448...|\n",
      "|           159|           4| [159.0,4.0]|[1.10152205440456...|\n",
      "|           263|           3| [263.0,3.0]|[1.82201446734843...|\n",
      "|           184|          45|[184.0,45.0]|[1.27471734597761...|\n",
      "|           223|          24|[223.0,24.0]|[1.54490200083156...|\n",
      "|           363|          24|[363.0,24.0]|[2.51479563364061...|\n",
      "|            91|          38| [91.0,38.0]|[0.63043086132588...|\n",
      "|           354|          48|[354.0,48.0]|[2.45244532867432...|\n",
      "|           163|          41|[163.0,41.0]|[1.12923330105625...|\n",
      "|           269|          44|[269.0,44.0]|[1.86358133732596...|\n",
      "|           371|          34|[371.0,34.0]|[2.57021812694399...|\n",
      "|           499|          25|[499.0,25.0]|[3.45697801979798...|\n",
      "|            24|          13| [24.0,13.0]|[0.16626747991012...|\n",
      "|           220|           5| [220.0,5.0]|[1.52411856584279...|\n",
      "|           187|          30|[187.0,30.0]|[1.29550078096637...|\n",
      "|            25|          13| [25.0,13.0]|[0.17319529157304...|\n",
      "|           117|          38|[117.0,38.0]|[0.81055396456185...|\n",
      "|           245|          26|[245.0,26.0]|[1.69731385741584...|\n",
      "|           275|          50|[275.0,50.0]|[1.90514820730349...|\n",
      "+--------------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea2c5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "wssse_k3 =model_k3.transform(cluster_final_data)\n",
    "wssse_k2 = model_k2.transform(cluster_final_data)\n",
    "wssse_k3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47dc09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+--------------------+----------+\n",
      "|quantiteAchete|prixUnitaire|    features|      scaledFeatures|prediction|\n",
      "+--------------+------------+------------+--------------------+----------+\n",
      "|           198|           9| [198.0,9.0]|[1.37170670925851...|         1|\n",
      "|           224|          16|[224.0,16.0]|[1.55182981249448...|         1|\n",
      "|           159|           4| [159.0,4.0]|[1.10152205440456...|         1|\n",
      "|           263|           3| [263.0,3.0]|[1.82201446734843...|         0|\n",
      "|           184|          45|[184.0,45.0]|[1.27471734597761...|         2|\n",
      "|           223|          24|[223.0,24.0]|[1.54490200083156...|         1|\n",
      "|           363|          24|[363.0,24.0]|[2.51479563364061...|         0|\n",
      "|            91|          38| [91.0,38.0]|[0.63043086132588...|         2|\n",
      "|           354|          48|[354.0,48.0]|[2.45244532867432...|         2|\n",
      "|           163|          41|[163.0,41.0]|[1.12923330105625...|         2|\n",
      "|           269|          44|[269.0,44.0]|[1.86358133732596...|         2|\n",
      "|           371|          34|[371.0,34.0]|[2.57021812694399...|         2|\n",
      "|           499|          25|[499.0,25.0]|[3.45697801979798...|         0|\n",
      "|            24|          13| [24.0,13.0]|[0.16626747991012...|         1|\n",
      "|           220|           5| [220.0,5.0]|[1.52411856584279...|         1|\n",
      "|           187|          30|[187.0,30.0]|[1.29550078096637...|         2|\n",
      "|            25|          13| [25.0,13.0]|[0.17319529157304...|         1|\n",
      "|           117|          38|[117.0,38.0]|[0.81055396456185...|         2|\n",
      "|           245|          26|[245.0,26.0]|[1.69731385741584...|         2|\n",
      "|           275|          50|[275.0,50.0]|[1.90514820730349...|         2|\n",
      "+--------------+------------+------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce4c792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with K=3\n",
      "within Set Sun Squared Errore= DataFrame[quantiteAchete: int, prixUnitaire: int, features: vector, scaledFeatures: vector, prediction: int]\n",
      "with K=2\n",
      "within Set Sum of Squared Errors a DataFrame[quantiteAchete: int, prixUnitaire: int, features: vector, scaledFeatures: vector, prediction: int]\n"
     ]
    }
   ],
   "source": [
    "print(\"with K=3\")\n",
    "print(\"within Set Sun Squared Errore= \"+ str(wssse_k3))\n",
    "\n",
    "print(\"with K=2\")\n",
    "print(\"within Set Sum of Squared Errors a \"+ str(wssse_k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a20a962",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o523.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 249.0 failed 1 times, most recent failure: Lost task 4.0 in stage 249.0 (TID 3208) (LAPTOP-RAGD177R executor driver): java.lang.NumberFormatException: For input string: \"2928346;Christopher;Prince;Magazine\"\r\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\r\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\r\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\r\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\r\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:131)\r\n\tat org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\r\n\tat org.apache.spark.mllib.util.MLUtils$.computeNumFeatures(MLUtils.scala:94)\r\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.$anonfun$inferSchema$1(LibSVMRelation.scala:105)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.inferSchema(LibSVMRelation.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NumberFormatException: For input string: \"2928346;Christopher;Prince;Magazine\"\r\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\r\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\r\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\r\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\r\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:131)\r\n\tat org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40132\\2276472371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Charger les données\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"libsvm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.dat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Initialiser l'algorithme K-Means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o523.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 249.0 failed 1 times, most recent failure: Lost task 4.0 in stage 249.0 (TID 3208) (LAPTOP-RAGD177R executor driver): java.lang.NumberFormatException: For input string: \"2928346;Christopher;Prince;Magazine\"\r\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\r\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\r\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\r\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\r\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:131)\r\n\tat org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\r\n\tat org.apache.spark.mllib.util.MLUtils$.computeNumFeatures(MLUtils.scala:94)\r\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.$anonfun$inferSchema$1(LibSVMRelation.scala:105)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.inferSchema(LibSVMRelation.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NumberFormatException: For input string: \"2928346;Christopher;Prince;Magazine\"\r\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\r\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\r\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\r\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:321)\r\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\r\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:131)\r\n\tat org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Charger les données\n",
    "data = spark.read.format(\"libsvm\").load(\"spark.dat\")\n",
    "\n",
    "# Initialiser l'algorithme K-Means\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "\n",
    "# Ajuster l'algorithme K-Means sur les données\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Prédire les clusters pour les données\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Afficher les résultats\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b10e864",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: quantiteAchete, prixUnitaire",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40132\\238894415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Ajuster l'algorithme de régression logistique sur les données d'entraînement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodele\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Prédire les résultats sur les données de test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: quantiteAchete, prixUnitaire"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "dataset_1=spark.read.csv(\n",
    "    path=\"spark.dat\",\n",
    "    sep=\";\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")\n",
    "dataset=dataset_1.select(['quantiteAchete','prixUnitaire'])\n",
    "\n",
    "trainingData,testData = dataset.randomSplit([0.7, 0.3], seed=1)\n",
    "\n",
    "# Initialiser l'algorithme de régression logistique\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Ajuster l'algorithme de régression logistique sur les données d'entraînement\n",
    "modele = lr.fit(dataset)\n",
    "\n",
    "# Prédire les résultats sur les données de test\n",
    "predictions_1 = modele.transform(testData)\n",
    "\n",
    "# Afficher les résultats\n",
    "predictions_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be70b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData,testData = dataset.randomSplit([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ff1eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser l'algorithme de régression logistique\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38b88dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: quantiteAchete, prixUnitaire",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40132\\11635030.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ajuster l'algorithme de régression logistique sur les données d'entraînement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodele\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: quantiteAchete, prixUnitaire"
     ]
    }
   ],
   "source": [
    "# Ajuster l'algorithme de régression logistique sur les données d'entraînement\n",
    "modele = lr.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e05df9ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40132\\468569129.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphLoader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medgeListFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"path/to/edge_list_file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'GraphLoader' is not defined"
     ]
    }
   ],
   "source": [
    "###### GRAPH X ######\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "037e376f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_86500\\2273305560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#val_graph = GraphLoader.edgeListFile(spark, \"spark.dat\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Initialiser une session Spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GraphX Example\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "#Charger un graphe depuis un fichier\n",
    "val_graph = GraphLoader.edgeListFile(spark, \"spark.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opérations de filtrage sur les sommets\n",
    "val_graph = GraphLoader.edgeListFile(spark, \"path/to/edge_list_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d26157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opétions de filtrage sur les arrêtes\n",
    "val filteredAretes = graph.edges.filter(e => e._attr >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aef05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compter le nombre de sommets et d'arrêtes \n",
    "val vertexCount = graph.vertices.count # Sommet\n",
    "val edgeCount = graph.edges.count #arrêtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62603e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trouver le degré moyen de sommets\n",
    "val avgDegree = graph.degrees.map(_._2).reduce(_ + _) / vertexCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trouver le nombre de composantes connexes\n",
    "val connectedComponents = graph.connectedComponents.vertices.map(_._2).countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trouver le plus court chemin entre 2 sommets\n",
    "val sources = Seq(1, 2)\n",
    "val destinations = Seq(10, 20)\n",
    "val distances = graph.shortestPaths.sources(sources).destinations(destinations).run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds ayant le plus grand nombre de voisins\n",
    "graph.degrees.sortBy(_._2, ascending=false).take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
